# 웹 로봇

웹 로봇은 사람과의 상호작용 없이 연속된 웹 트랜잭션들을 자동으로 수행하는 소프트웨어 프로그램이다. 많은 로봇이 웹 사이트에서 다른 웹 사이트로 떠돌아 다니면서,
콘텐츠를 가져오고 하이퍼링크를 따라가고, 그들이 발견한 데이터를 처리한다. 이러한 종류의 로봇들은 마치 스스로 마음을 가지고 있는 것처럼 자동으로 웹 
사이트들을 탐색하며, 그 방식에 따라 `크롤러, 스파이더, 웜, 봇`등 각양각색의 이름으로 불린다.

- 웹 로봇 Example
  - 주식시장 서버에 매 분 HTTP GET 요청을 보내고, 여기서 얻은 데이터를 활용해 주가 추이 그래프를 생성하는 주식 그래프 로봇
  - 검색 데이터베이스를 만들기 위해 발견한 모든 문서를 수집하는 검색엔진 로봇
  - 상품에 대한 가격 데이터베이스를 만들기 위해 온라인 쇼핑몰의 카탈로그에서 웹 페이지를 수집하는 가격 비교 로봇
  
## 크롤러와 크롤링

웹 크롤러는 먼저 웹 페이지를 한 개 가져오고, 그 페이지가 가리키는 모든 웹 페이지를 가져오고, 다시 그 페이지들이 가리키는 모든 웹 페이지들을 가져오는
(이러한 일을 재귀적으로 반복하는 방식) 방식으로 웹을 순회하는 로봇이다.

웹 링크를 재귀적으로 따라가는 로봇을 `크롤러` 혹은 `스파이더`라고 부르는데, HTML 하이퍼링크들로 만들어진 웹을 따라 `기어다니기(crawl)` 때문이다.

## 루프와 중복 피하기

웹은 로봇이 문제를 일으킬 가능성으로 가득 차 있다. 이러한 웹에서 로봇이 더 올바르게 동작하기 위해 사용하는 기법들은 다음과 같다.

- URL 정규화
- 너비 우선 크롤링
- 스로틀링
- URL 크기 제한
- URL/사이트 블랙리스트
- 패턴 발견
- 콘텐츠 지문(fingerprint)
- 사람의 모니터링

## robots.txt

웹 마스터에게 로봇의 동작을 더 잘 제어할 수 있는 매커니즘을 제공하는 단순하고 자발적인 기법이 제안되었다. 이 표준은 `Robots Exclusion Standard(로봇 차단 표준)`
라는 이름이 지어졌지만, 로봇의 접근을 제어하는 정보를 저장하는 파일의 이름을 따서 종종 그냥 `robots.txt`라고 불린다.

robots.txt의 아이디어는 단순하다.

어떤 웹 서버는 서버의 문서 루트에 robots.txt 라고 이름이 붙은 선택적인 파일을 제공할 수 있다. 이 파일은 어떤 로봇이 서버의 어떤 부분에 접근할 수 있는지에 대한 저옵가
담겨져 있다. 만약 어떤 로봇이 자발적인 표준에 따른다면, 그것은 웹 사이트의 어떤 다른 리소스에 접근하기 전에 우선 그 사이트의 robots.txt를 요청할 것이다.

예를들어 웹 로봇이 http://www.naver.com/specials/lego.html을 다운로드 받으려고한다. 하지만 로봇은 그 페이지를 요청하기 전에 먼저 이페이지를 가져올 수 잇는
권한이 있는지 확인하기 위해 robots.txt 파일을 검사할 필요가 있다. 이 예에서, robots.txt 파일은 로봇을 차단하지 않으므로 로봇은 그 페이지를 가져오게 된다.

> 즉, 로봇이 리소스를 요청하기 전에, 리소스에 접근할 수 있는지에 대한 권한을 서버가 가지고 있는 robots.txt 파일을 통해서 확인하게 된다.

웹 사이트의 어떤 URL을 방문하기 전에, 그 웹 사이트에 robots.txt 파일이 존재한다면 로봇은 반드시 그 파일을 가져와서 처리해야 한다. 호스트 명과 포트번호에 의해 정의도는 
어떤 웹 사이트가 있을 때, 그 사이트 전체에 대한 robots.txt 파일은 단 하나만이 존재한다.

### robots.txt 가져오기

로봇은 웹 서버의 여느 파일들과 마찬가지로 HTTP GET 메서드를 이요해 robots.txt 리소스를 가져온다. robots.txt 리소스가 존재한다면 서버는 그 파일을
`text/plain` 본문으로 반환한다. 만약 서버가 `404 Not Fount HTTP` 상태코드로 응답한다면 로봇은 그 서버는 로봇의 접근을 제한하지 않는 것으로 간주하고 어던 파일이든
요청하게 될 것이다.

로봇은 사이트 관리자가 로봇의 접근을 추적할 수 있도록 From이나 User-Agent 헤더를 통해 신원 정보를 넘긴다.

### robots.txt 파일 포맷

robots.txt 파일의 각 줄은 빈줄, 주석 줄, 규칙 줄의 세 가지 종류가 있다. 규칙 줄은 HTTP 헤더처럼 생겼고(필드:값) 패턴 매칭을 위해 사용된다.

- EXAMPLE

```
# 이 robots.txt 파일은 Slurp와 WebCrawler가 우리 사이트의 공개된 영역을 크롤링하는 것을 허락한다. 
# 다른 로봇들은 안된다.

User-Agent: slurp
User-Agent webcrawler
Disallow: /private

User-Agent: *
Disallow:
```

robots.txt의 줄은 레코드로 구분된다. 각 레코드는 특정 로봇들의 집합에 대한 차단 규칙의 집합을 기술한다. 

이 방법을 통해 로봇별로 각각 다른 차단 규칙을 적용할 수 있다.

각 레코드는 규칙 줄들의 집합으로 되어 있으며 빈 줄이나 파일 끝(end of file) 문자로 끝난다. 

위 파일은 Slurp와 Webcrawler 로봇들이 사적인 하위 디렉터리 안에 있는 것을 제외한 어떤 파일이든 접근할 수 있도록 허용하는 것이다.

- 형식
  - User-Agent : `<robot-name>`
  - User-Agent : * 
    - 로봇 이름이 `*`인 레코들 중 첫 번째 것
    - 만약 위에 대응하는 User-Agent 줄을 찾지못하면 접근에 제한이 없다.
  - Disallow와 Allow 줄은 로봇 차단 레코드의 User-Agent 줄들 바로 다음에 온다.
  - 이 줄들은 특정 로봇에 대해 어떤 URL 경로가 명시적으로 금지되어 있고 명시적으로 허용되는지 기술한다.
  - 예를들어 `Disallow:/tmp`는 다음 모든 URL에 대응된다.
    - http://www.naver.com/tmp
    - http://www.naver.com/tmp/
    - http://www.naver.com/tmp/pliers.html
    - http://www.naver.com/tmp/stuff.txt
    
#### 그 외 규칙들

- 로봇은 자신이 이해하지 못하는 필드는 무시해야한다.
- 하위 호환성을 위해, 한 줄을 여러 줄로 나누어 적는것은 허용되지 않는다.
- 주석(#)은 파일의 어디에서든 허용된다.

robots.txt 파일의 캐싱을 제어하기 위해 표준 HTTP 캐시 제어 매커니즘이 원 서버와 로봇 양쪽 모두에 의해 사용된다. 로봇은 HTTP 응답의 
`Cache-Control`과 `Expires` 헤더에 주의를 기울여야 한다.

#### 로봇 차단 펄 코드
  
robots.txt 파일과 상호작용하는 공개된 펄(Perl) 라이브러리가 몇 가지 존재한다.

## 검색엔진

웹 로봇을 가장 광범위하게 사용하는 것은 인터넷 검색엔진이다. 인터넷 검색엔진은 사용자가 전 세계의 어떤 주제에 대한 문서라도 찾을 수 있게 해준다.
오늘날의 유명한 웹 사이트들의 상당수가 검색엔진이다. 

검색엔진은 수십억 개의 웹 페이지들을 검색하기 위해 복잡한 크롤러를 사용해야 한다.

### 현대적인 검색엔진의 아키텍처

오늘날 검색엔진들은 그들이 갖고 있는 전 세계의 웹 페이지들에 대해 `풀 텍스트 색인(full-text indexes)`이라고 하는 복잡한 로컬 데이터베이스를 생성한다.
이 색인은 웹 의 모든 문서에 대한 일종의 카드 카탈로그처럼 동작한다.

검색엔진 크롤러들은 웹 페이지들을 수집하여 집으로 가져와서, 이 풀 텍스트 색인에 추가한다. 동시에, 검색엔진 사용자들은 핫봇이나 구글과 같은 웹 검색
게이트웨이를 통해 풀 텍스트 색인에 대한 질의를 보낸다. 크롤링을 한 번하는데 걸리는 시간이 상당한 데 비해 웹 페이지들은 매 순간 변화하기 때문에, 풀 텍스트 색인은
기껏 해봐야 웹의 특정 순간에 대한 스냅샷에 불과하다.

> 사용자들 -> 웹 검색 게이트웨이 <-> 풀 텍스트 색인 데이터베이스 <-> 검색엔진 크롤러/색인기 <- 웹 서버들

### 풀 텍스트 색인(full text indexes)

풀 텍스트 색인은 단어 하나를 입력받아 그 단어를 포함하고 있는 문서를 즉각 알려줄 수 있는 데이터베이스이다. 이 문서들은 색인이 생성된 이후에는
검색할 필요가 없다.

### 질의 보내기

사용자가 질의를 웹 검색엔진 게이트웨이로 보내는 방법은, HTML 폼을 사용자가 채워 넣고, 브라우저가 그 폼을 HTTP GET이나 POST 요청을 이용해서
게이트웨이로 보내는 식이다. 게이트웨이 프로그램은 검색 질의를 추출하고 웹 UI 질의를 풀 텍스트 색인을 검색할 때 사용되는 표현식으로 변환한다.

예를들어 사용자가 검색창(input text)에 http라고 검색하면 브라우저는 이를 쿼리 스트링을 URL의 일부로 포함하는 GET 요청으로 번역한다.

`GET /search.html?query=http HTTP/1.1`

검색 결과가 상당히 많이 나올 수 있는데, 결과 문서들 간의 순서를 알 필요가 있다. 이것은 `관련도 랭킹(relevancy ranking)`이라고 불리며, 검색 결과의
목록에 점수를 매기고 정렬하는 과정이다.
  
